# -*- coding: utf-8 -*-
"""Support Vector Machine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16JHMfrgD7BSxaAj7wa04qMcQ66YXnG-n
"""

#Spam detection using Support Vector Machine in Python

import pandas as pd

import matplotlib.pyplot as plt

import numpy as np 

# Algorithms
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

#Load the data
from google.colab import files
uploaded = files.upload()

#Read the CSV file
df = pd.read_csv('spam_ham_dataset.csv')

#Print the first 10 rows of data
df.head(10)

#Rename columns as category and message
df.rename(columns = {'Unnamed: 0':'Random', 'label':'Category', 'label_num': 'TrueorFalse'}, inplace = True)
df.head(10)

print(f'Dataset consist of {df.shape[0]} E-Mails.')

df['Category'].value_counts()

plt.figure(figsize=(8,6))

df['Category'].value_counts().plot.bar(color = ["orange","orange"])
plt.title('Total number of ham and spam in the dataset')
plt.show()

#0: Ham, 1: Spam
df['Category']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)
df.head()

#0: Ham, 1: Spam
df['Category']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)
df.head()

X=df['text']
Y=df['Category']

X_train, X_test, y_train, y_test = train_test_split(X,Y)

#Support Vector Machine

#SVM distinguishes classes by drawing a decision boundary. How to draw or determine the decision boundary is the most critical part in SVM algorithms.
#Before creating the decision boundary, each observation (or data point) is plotted in n-dimensional space. “n” is the number of features used. For instance, if we use “length” and “width” to classify different “cells”, observations are plotted in a 2-dimensional space and decision boundary is a line. 
#If we use 3 features, decision boundary is a plane in 3-dimensional space. If we use more than 3 features, decision boundary becomes a hyperplane which is really hard to visualize.

#Decision boundary is drawn in a way that the distance to support vectors are maximized. If the decision boundary is too close to a support vector, it will be highly sensitive to noises and not generalize well. Even very small changes in independent variables may cause a misclassification.

#The data points are not always linearly separable like in the figure above. In these cases, SVM uses kernel trick which measures the similarity (or closeness) of data points in a higher dimensional space in order to make them linearly separable.

#Kernel function is kind of a similarity measure. The inputs are original features and the output is a similarity measure in the new feature space.
#Similarity here means a degree of closeness. It is a costly operation to actually transform data points to a high-dimensional feature space. 
#The algorithm does not actually transform the data points to a new, high dimensional feature space. Kernelized SVM compute decision boundaries in terms of similarity measures in a high-dimensional feature space without actually doing a transformation. I think this is why it is also called kernel trick.

#SVM is especially effective in cases where number of dimensions are more than the number of samples. When finding the decision boundary, SVM uses a subset of training points rather than all points which makes it memory efficient. On the other hand, training time increases for large datasets which negatively effects the performance.

clf_svm= Pipeline([
    ('vectorizer', CountVectorizer()),
    ('svc', SVC(kernel="rbf",C=1000,gamma=0.001))
])

clf_svm.fit(X_train,y_train)

y_pred_SVM=clf_svm.predict(X_test)

svm_acc=accuracy_score(y_test,y_pred_SVM)
svm_acc

#Test
def spam_dect(clf,txt):
    a=clf.predict([txt])
    if a==1:
        print(f"{clf[1]} This is a Spam email \n")
    else:
        print(f"{clf[1]} This is a Real email \n")

message= input()

clf_1 = clf_svm
i = [clf_1]
for x in i:
  spam_dect(x,message)