# -*- coding: utf-8 -*-
"""Copy of 03/12/22SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1joaWMmL-Tj4-5p5h_nirQSfFXwG20bAw
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import string
import nltk

# Algorithm
import string
import nltk
from sklearn import svm, datasets
from nltk.corpus import stopwords
from sklearn.svm import SVC   #Support Vector Classifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

#Load the data
from google.colab import files
uploaded = files.upload()

#Read the CSV file
df = pd.read_csv('spam_ham_dataset.csv')

#Print the first 10 rows of data
df.head(10)

# Drop unnecesery and empty colums
cols = [0,3]
df.drop(df.columns[cols],axis=1,inplace=True)
df.head(10)

#Rename columns as category and message
df.rename(columns = {'label':'Category', 'text':'Message'}, inplace = True)
df.head(10)

#Get the number of rows and columns as a tuple
print(f'Dataset consist of {df.shape[0]} E-Mails.')

#Get the total number of ham and spam dataset
df['Category'].value_counts()

plt.figure(figsize=(8,6))

df['Category'].value_counts().plot.bar(color = ["orange","orange"])
plt.title('Total number of ham and spam in the dataset')
plt.show()

from wordcloud import WordCloud

plt.figure(figsize = (15,15)) 
wc = WordCloud(max_words = 2000 , width = 1000 , height = 500).generate(" ".join(df[df.Category =="ham" ].Message))
plt.imshow(wc , interpolation = 'bilinear')
plt.title("Ham Word Cloud")

plt.figure(figsize = (15,15)) 
wc = WordCloud(max_words = 2000 , width = 1000 , height = 500).generate(" ".join(df[df.Category =="spam" ].Message))
plt.imshow(wc , interpolation = 'bilinear')
plt.title("Spam Word Cloud")

#0: Ham, 1: Spam
df['Category']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)
df.head()

X=df['Message']
Y=df['Category']

#Download the stopwords package
nltk.download('stopwords')

def process_text(text):
  #1 Remove the punctuation
  #2 Remove stopwords
  #3 return a list of a clean text words

  #1
  nopunc = [char for char in text if char not in string.punctuation]
  nopunc = ''.join(nopunc)

  #2
  clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
  
  #3
  return clean_words

# Show the tokenization (a list of tokens also called lemmas)
df['Message'].head().apply(process_text)

#Convert a collection of text to a matrix of tokens
from sklearn.feature_extraction.text import CountVectorizer
messages_bow = CountVectorizer(analyzer=process_text).fit_transform(df['Message'])

#Split the data into %80 training and %20 testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(messages_bow, df['Category'], test_size = 0.3, random_state= 109)

#Get the shape of messages_bow (5171 row of data and 50381 colums in our data set)
messages_bow.shape

X_train, X_test, y_train, y_test = train_test_split(X,Y)

clf_svm= Pipeline([
    ('vectorizer', CountVectorizer()),
    ('svc', SVC(kernel="rbf",C=1000,gamma=0.001))
])

clf_svm.fit(X_train,y_train)

y_pred_SVM=clf_svm.predict(X_test)

svm_acc=accuracy_score(y_test,y_pred_SVM)
svm_acc

def spam_dect(clf,txt):
    a=clf.predict([txt])
    if a==1:
        print(f"{clf[1]} This is a Spam email \n")
    else:
        print(f"{clf[1]} This is a Real email \n")

message= input()

clf_2 = clf_svm
i = [clf_2]
for x in i:
    spam_dect(x,message)

"""DENEME"""

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2] # we only take the first two features. We could
 # avoid this ugly slicing by using a two-dim dataset
y = iris.target

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0 # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=1,gamma=0.001).fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
h = (x_max / x_min)/100
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
 np.arange(y_min, y_max, h))

plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()

"""DENEME2"""